FROM fluxcapacitor/package-gpu-cuda8-16.04:master

WORKDIR /root

RUN \
  apt-get install -y software-properties-common \
  && add-apt-repository -y ppa:openjdk-r/ppa \
  && apt-get update \
  && apt-get install -y --no-install-recommends openjdk-8-jdk openjdk-8-jre-headless \
  && apt-get install -y apt-transport-https \
  && apt-get install -y wget \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*

ENV \
  JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/

ENV \
  BAZEL_VERSION=0.4.5 \
  TENSORFLOW_SERVING_VERSION=0.5.1
 
# TensorFlow Serving Home (not required on PATH)
ENV \
  TENSORFLOW_SERVING_HOME=/root/serving \
  TENSORFLOW_HOME=/root/serving/tensorflow

# Required by TensorFlow Serving
RUN \
 apt-get update \
 && apt-get install -y \
        build-essential \
        curl \
        libcurl3-dev \
        git \
        libfreetype6-dev \
        libpng12-dev \
        libzmq3-dev \
        pkg-config \
        python-dev \
        python-numpy \
        python-pip \
        software-properties-common \
        swig \
        zip \
        zlib1g-dev

RUN \
  pip install grpcio

# Install Python with conda
RUN wget -q https://repo.continuum.io/miniconda/Miniconda3-4.1.11-Linux-x86_64.sh -O /tmp/miniconda.sh  && \
    echo '874dbb0d3c7ec665adf7231bbb575ab2 */tmp/miniconda.sh' | md5sum -c - && \
    bash /tmp/miniconda.sh -f -b -p /opt/conda && \
    /opt/conda/bin/conda install --yes python=3.5 sqlalchemy tornado jinja2 traitlets requests pip && \
    /opt/conda/bin/pip install --upgrade pip && \
    rm /tmp/miniconda.sh

ENV \
  PATH=/opt/conda/bin:$PATH

RUN \
  conda install --yes openblas scikit-learn numpy scipy matplotlib pandas seaborn

RUN \
  apt-get install -y python-qt4

RUN \ 
  mkdir /root/bazel \
  && cd /root/bazel \
  && curl -fSsL -O https://github.com/bazelbuild/bazel/releases/download/$BAZEL_VERSION/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh \
  && curl -fSsL -o /root/bazel/LICENSE.txt https://raw.githubusercontent.com/bazelbuild/bazel/master/LICENSE.txt \
  && chmod +x bazel-*.sh \
  && ./bazel-$BAZEL_VERSION-installer-linux-x86_64.sh \
  && rm bazel-$BAZEL_VERSION-installer-linux-x86_64.sh

# Clone Tensorflow Serving and the Tensorflow Submodule
RUN \
 cd ~ \
 && git clone -b master --single-branch --recurse-submodules https://github.com/tensorflow/serving.git \
 && cd $TENSORFLOW_SERVING_HOME \
 && git reset --hard 4d0a571ff9c15b937f58d3d5e97a5310b5decf2b
   # be83fe98104f26362dd2819d20b53ffac9b8f592

ENV TF_NEED_CUDA=1
ENV TF_NEED_GCP=0
ENV TF_NEED_JEMALLOC=1
ENV TF_NEED_HDFS=1
ENV TF_NEED_OPENCL=0
ENV TF_ENABLE_XLA=0
ENV TF_CUDA_VERSION=8.0
ENV TF_CUDNN_VERSION=5
ENV CUDA_PATH="/usr/local/cuda"
ENV CUDA_TOOLKIT_PATH=$CUDA_PATH
ENV CUDNN_INSTALL_PATH=$CUDA_PATH
ENV CC_OPT_FLAGS="-march=native"
ENV TF_CUDA_COMPUTE_CAPABILITIES=3.7
# Check the required COMPUTE_CAPABILITIES from the following link:
#  https://developer.nvidia.com/cuda-gpus
# Also, Tensorflow has a minimum-supported COMPUTE CAPABILITY (ie. 3.5)
# ie. here are the AWS and GCP Instance Types and their COMPUTE CAPABILITIES
#
####################
###      AWS     ###
##  P2 Instances  ##
# Tesla K-80 (3.7) #
#                  #
##  G2 Instances  ##
# GRID K520 (3.5)  #
#                  #
###  Google GCP  ###   
# Tesla K-80 (3.7) #
####################

ENV \
  LANG=en_us.UTF-8

RUN \
  cd $TENSORFLOW_HOME \
  && printf "\n\n\n" | ./configure 

RUN \  
  cd $TENSORFLOW_SERVING_HOME \
  && sed -i.bak 's/@org_tensorflow\/\/third_party\/gpus\/crosstool/@local_config_cuda\/\/crosstool:toolchain/g' tools/bazel.rc \
  && sed -i.bak '/nccl/d' tensorflow/tensorflow/contrib/BUILD \
# http://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions
  && bazel build --local_resources 5000,1.0,1.0 -c opt --config=cuda --spawn_strategy=standalone --genrule_strategy=standalone --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.1 --copt=-msse4.2 -k //tensorflow_serving/... \
  && rm -rf /root/.cache/bazel

ENV \
  PATH=$TENSORFLOW_SERVING_HOME/bazel-bin/tensorflow_serving/model_servers/:$PATH

# Configure the build for our CUDA configuration.
ENV CI_BUILD_PYTHON python
ENV TF_ENABLE_XLA=1

# Need this inside Docker for nvidia-docker build step HACK
ENV LD_LIBRARY_PATH /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH
RUN \
  mkdir -p /usr/local/nvidia/lib64/ \
  && cd /usr/local/nvidia/lib64/ \
  && ln -s /usr/local/cuda-8.0/targets/x86_64-linux/lib/stubs/libcuda.so libcuda.so.1
RUN \
  ldconfig /usr/local/cuda/lib64

#RUN \
#  cd $TENSORFLOW_HOME \
#  && tensorflow/tools/ci_build/builds/configured GPU \
#  && bazel build --local_resources 5000,1.0,1.0 -c opt --config=cuda --spawn_strategy=standalone --genrule_strategy=standalone --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.1 --copt=-msse4.2 -k //tensorflow/... \
#  && rm -rf /root/.cache/bazel

# This configuration was inspired by the following resources:
#   http://ci.tensorflow.org/job/tensorflow-master-linux-xla/104/consoleText
#   https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build
#   tensorflow/tools/ci_build/xla/linux/gpu/run_py3.sh

# Notes:  --local_test_jobs should be sync'd with the TF_GPU_COUNT
#         Also, we may need to just call `bazel build`,
#           since we may not be building on a GPU.
#         The commit hash of the CI build above was bbe056e5a0ab81b67fcb6053400812b3d5805fc7,
#           but we tested with 12a98726e769e988f6368a029ec2f5b0ac3ccbd4 
#           (version used by this version of TF Serving) and things look OK.

# TODO:  Make these dynamic based on build worker
ENV TF_GPU_COUNT=1
ENV N_JOBS=8
#$(grep -c ^processor /proc/cpuinfo)

RUN \
  cd $TENSORFLOW_HOME \
  && printf "\n\n\n" | ./configure

RUN \
  cd $TENSORFLOW_HOME \
  && bazel build --local_resources 5000,1.0,1.0 --config=cuda \
    --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.1 --copt=-msse4.2 \
    --test_tag_filters=-no_gpu,-benchmark-test -k \
    --jobs=${N_JOBS} \
    --test_timeout 300,450,1200,3600 \
#   --build_tests_only \
    --test_output=errors \
    --local_test_jobs=${TF_GPU_COUNT} \
    --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute -- \
    //tensorflow/... \
    && rm -rf /root/.cache/bazel

RUN \
  cd $TENSORFLOW_HOME \
  && bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip

RUN \
  pip --no-cache-dir install --ignore-installed --upgrade /tmp/pip/tensorflow-*.whl

RUN \
  TF_CPP_MIN_LOG_LEVEL=0 \
  TF_XLA_FLAGS=--xla_generate_hlo_graph=.*

# Cleanup for nvidia-docker build step HACK
RUN \
  rm /usr/local/nvidia/lib64/libcuda.so.1
