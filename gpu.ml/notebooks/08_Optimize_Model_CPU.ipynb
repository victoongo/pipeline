{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Transform Tool\n",
    "\n",
    "https://petewarden.com/2016/12/30/rewriting-tensorflow-graphs-with-the-gtt/\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Trained Models for Inference\n",
    "## Types of Optimizations\n",
    "* Remove training-only operations (checkpoint saving, drop out)\n",
    "* Strip out unused nodes\n",
    "* Remove debug operations\n",
    "* Fold batch normalization ops into weights (super cool)\n",
    "* Round weights\n",
    "* Quantize weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Types of Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Model (CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -l /root/models/linear/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "summarize_graph --in_graph=/root/models/optimize_me/unoptimized_cpu.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "benchmark_model --graph=/root/models/optimize_me/unoptimized_cpu.pb \\\n",
    "    --input_layer=weights,bias,x_observed \\\n",
    "    --input_layer_type=float,float,float \\\n",
    "    --input_layer_shape=:: \\\n",
    "    --output_layer=add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip Unused Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "transform_graph \\\n",
    "    --in_graph=/root/models/optimize_me/unoptimized_cpu.pb \\\n",
    "    --out_graph=/root/models/optimize_me/strip_unused_optimized_cpu.pb \\\n",
    "    --inputs='x_observed,weights,bias' \\\n",
    "    --outputs='add' \\\n",
    "    --transforms='strip_unused_nodes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -l /root/models/optimize_me/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "summarize_graph --in_graph=/root/models/optimize_me/strip_unused_optimized_cpu.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import re\n",
    "from google.protobuf import text_format\n",
    "from tensorflow.core.framework import graph_pb2\n",
    "\n",
    "def convert_graph_to_dot(input_graph, output_dot, is_input_graph_binary):\n",
    "    graph = graph_pb2.GraphDef()\n",
    "    with open(input_graph, \"rb\") as fh:\n",
    "        if is_input_graph_binary:\n",
    "            graph.ParseFromString(fh.read())\n",
    "        else:\n",
    "            text_format.Merge(fh.read(), graph)\n",
    "    with open(output_dot, \"wt\") as fh:\n",
    "        print(\"digraph graphname {\", file=fh)\n",
    "        for node in graph.node:\n",
    "            output_name = node.name\n",
    "            print(\"  \\\"\" + output_name + \"\\\" [label=\\\"\" + node.op + \"\\\"];\", file=fh)\n",
    "            for input_full_name in node.input:\n",
    "                parts = input_full_name.split(\":\")\n",
    "                input_name = re.sub(r\"^\\^\", \"\", parts[0])\n",
    "                print(\"  \\\"\" + input_name + \"\\\" -> \\\"\" + output_name + \"\\\";\", file=fh)\n",
    "        print(\"}\", file=fh)\n",
    "        print(\"Created dot file '%s' for graph '%s'.\" % (output_dot, input_graph))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_graph='/root/models/optimize_me/strip_unused_optimized_cpu.pb'\n",
    "output_dot='/root/notebooks/strip_unused_optimized_cpu.dot'\n",
    "convert_graph_to_dot(input_graph=input_graph, output_dot=output_dot, is_input_graph_binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dot -T png /root/notebooks/strip_unused_optimized_cpu.dot \\\n",
    "    -o /root/notebooks/strip_unused_optimized_cpu.png > /tmp/a.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image('/root/notebooks/strip_unused_optimized_cpu.png', width=1024, height=768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "benchmark_model --graph=/root/models/optimize_me/strip_unused_optimized_cpu.pb \\\n",
    "    --input_layer=weights,bias,x_observed \\\n",
    "    --input_layer_type=float,float,float \\\n",
    "    --input_layer_shape=:: \\\n",
    "    --output_layer=add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fold Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "transform_graph \\\n",
    "--in_graph=/root/models/optimize_me/unoptimized_cpu.pb \\\n",
    "--out_graph=/root/models/optimize_me/fold_constants_optimized_cpu.pb \\\n",
    "--inputs='x_observed' \\\n",
    "--outputs='add' \\\n",
    "--transforms='\n",
    "fold_constants(ignore_errors=true)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -l /root/models/optimize_me/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "summarize_graph --in_graph=/root/models/optimize_me/fold_constants_optimized_cpu.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_graph='/root/models/optimize_me/fold_constants_optimized_cpu.pb'\n",
    "output_dot='/root/notebooks/fold_constants_optimized_cpu.dot'\n",
    "convert_graph_to_dot(input_graph=input_graph, output_dot=output_dot, is_input_graph_binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dot -T png /root/notebooks/fold_constants_optimized_cpu.dot \\\n",
    "    -o /root/notebooks/fold_constants_optimized_cpu.png > /tmp/a.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image('/root/notebooks/fold_constants_optimized_cpu.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "benchmark_model --graph=/root/models/optimize_me/fold_constants_optimized_cpu.pb \\\n",
    "                --input_layer=x_observed,bias,weights \\\n",
    "                --input_layer_type=float,float,float \\\n",
    "                --input_layer_shape=:: \\\n",
    "                --output_layer=add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fold Batch Normalizations\n",
    "Must run Fold Constants first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "transform_graph \\\n",
    "--in_graph=/root/models/optimize_me/fold_constants_optimized_cpu.pb \\\n",
    "--out_graph=/root/models/optimize_me/fold_batch_norms_optimized_cpu.pb \\\n",
    "--inputs='x_observed' \\\n",
    "--outputs='add' \\\n",
    "--transforms='\n",
    "fold_batch_norms\n",
    "fold_old_batch_norms'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -l /root/models/optimize_me/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "summarize_graph --in_graph=/root/models/optimize_me/fold_batch_norms_optimized_cpu.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_graph='/root/models/optimize_me/fold_batch_norms_optimized_cpu.pb'\n",
    "output_dot='/root/notebooks/fold_batch_norms_optimized_cpu.dot'\n",
    "convert_graph_to_dot(input_graph=input_graph, output_dot=output_dot, is_input_graph_binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dot -T png /root/notebooks/fold_batch_norms_optimized_cpu.dot \\\n",
    "    -o /root/notebooks/fold_batch_norms_optimized_cpu.png > /tmp/a.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image('/root/notebooks/fold_batch_norms_optimized_cpu.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "benchmark_model --graph=/root/models/optimize_me/fold_batch_norms_optimized_cpu.pb \\\n",
    "                --input_layer=x_observed,bias,weights \\\n",
    "                --input_layer_type=float,float,float \\\n",
    "                --input_layer_shape=:: \\\n",
    "                --output_layer=add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize Weights\n",
    "Should run Fold Batch Normalization first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "transform_graph \\\n",
    "--in_graph=/root/models/optimize_me/fold_batch_norms_optimized_cpu.pb \\\n",
    "--out_graph=/root/models/optimize_me/quantize_weights_optimized_cpu.pb \\\n",
    "--inputs='x_observed' \\\n",
    "--outputs='add' \\\n",
    "--transforms='quantize_weights'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -l /root/models/optimize_me/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "summarize_graph --in_graph=/root/models/optimize_me/quantize_weights_optimized_cpu.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_graph='/root/models/optimize_me/quantize_weights_optimized_cpu.pb'\n",
    "output_dot='/root/notebooks/quantize_weights_optimized_cpu.dot'\n",
    "convert_graph_to_dot(input_graph=input_graph, output_dot=output_dot, is_input_graph_binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dot -T png /root/notebooks/quantize_weights_optimized_cpu.dot \\\n",
    "    -o /root/notebooks/quantize_weights_optimized_cpu.png > /tmp/a.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image('/root/notebooks/quantize_weights_optimized_cpu.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "benchmark_model --graph=/root/models/optimize_me/quantize_weights_optimized_cpu.pb --input_layer=x_observed,bias,weights --input_layer_type=float,float,float --input_layer_shape=:: --output_layer=add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "transform_graph \\\n",
    "--in_graph=/root/models/optimize_me/fold_batch_norms_optimized_cpu.pb \\\n",
    "--out_graph=/root/models/optimize_me/quantize_nodes_optimized_cpu.pb \\\n",
    "--inputs='x_observed' \\\n",
    "--outputs='add' \\\n",
    "--transforms='quantize_nodes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -l /root/models/optimize_me/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "summarize_graph --in_graph=/root/models/optimize_me/quantize_nodes_optimized_cpu.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_graph='/root/models/optimize_me/quantize_nodes_optimized_cpu.pb'\n",
    "output_dot='/root/notebooks/quantize_nodes_optimized_cpu.dot'\n",
    "convert_graph_to_dot(input_graph=input_graph, output_dot=output_dot, is_input_graph_binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dot -T png /root/notebooks/quantize_nodes_optimized_cpu.dot \\\n",
    "    -o /root/notebooks/quantize_nodes_optimized_cpu.png > /tmp/a.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image('/root/notebooks/quantize_nodes_optimized_cpu.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "benchmark_model --graph=/root/models/optimize_me/quantize_nodes_optimized_cpu.pb \\\n",
    "                --input_layer=x_observed,bias,weights \\\n",
    "                --input_layer_type=float,float,float \\\n",
    "                --input_layer_shape=:: \\\n",
    "                --output_layer=add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort by Execution Order\n",
    "* aka. Topological Order Sort\n",
    "* Minimize inference overhead \n",
    "* Inputs for a each node are guaranteed to be available on the forward path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "transform_graph \\\n",
    "--in_graph=/root/models/optimize_me/unoptimized_cpu.pb \\\n",
    "--out_graph=/root/models/optimize_me/sort_by_execution_order_optimized_cpu.pb \\\n",
    "--inputs='x_observed' \\\n",
    "--outputs='add' \\\n",
    "--transforms='\n",
    "sort_by_execution_order'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -l /root/models/optimize_me/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "summarize_graph --in_graph=/root/models/optimize_me/sort_by_execution_order_optimized_cpu.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "benchmark_model --graph=/root/models/optimize_me/sort_by_execution_order_optimized_cpu.pb \\\n",
    "    --input_layer=x_observed,bias,weights \\\n",
    "    --input_layer_type=float,float,float \\\n",
    "    --input_layer_shape=:: \\\n",
    "    --output_layer=add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine All Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "transform_graph \\\n",
    "--in_graph=/root/models/optimize_me/unoptimized_cpu.pb \\\n",
    "--out_graph=/root/models/optimize_me/fully_optimized_cpu.pb \\\n",
    "--inputs='x_observed' \\\n",
    "--outputs='add' \\\n",
    "--transforms='\n",
    "add_default_attributes\n",
    "remove_nodes(op=Identity, op=CheckNumerics)\n",
    "fold_constants(ignore_errors=true)\n",
    "fold_batch_norms\n",
    "fold_old_batch_norms\n",
    "quantize_weights\n",
    "quantize_nodes\n",
    "strip_unused_nodes\n",
    "sort_by_execution_order'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -l /root/models/optimize_me/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "summarize_graph --in_graph=/root/models/optimize_me/fully_optimized_cpu.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_graph='/root/models/optimize_me/fully_optimized_cpu.pb'\n",
    "output_dot='/root/notebooks/fully_optimized_cpu.dot'\n",
    "convert_graph_to_dot(input_graph=input_graph, output_dot=output_dot, is_input_graph_binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dot -T png /root/notebooks/fully_optimized_cpu.dot \\\n",
    "    -o /root/notebooks/fully_optimized_cpu.png > /tmp/a.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image('/root/notebooks/fully_optimized_cpu.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "benchmark_model --graph=/root/models/optimize_me/fully_optimized_cpu.pb \\\n",
    "    --input_layer=weights,x_observed,bias \\\n",
    "    --input_layer_type=float,float,float \\\n",
    "    --input_layer_shape=:: \\\n",
    "    --output_layer=add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obfuscate Names\n",
    "* Shorten and mangle internal graph node names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "transform_graph \\\n",
    "--in_graph=/root/models/optimize_me/fully_optimized_cpu.pb \\\n",
    "--out_graph=/root/models/optimize_me/obfuscate_names_optimized_cpu.pb \\\n",
    "--inputs='x_observed' \\\n",
    "--outputs='add' \\\n",
    "--transforms='\n",
    "obfuscate_names'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -l /root/models/optimize_me/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "summarize_graph --in_graph=/root/models/optimize_me/obfuscate_names_optimized_cpu.pb"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
